{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-03T13:04:34.060108Z",
     "start_time": "2024-07-03T13:04:34.056280Z"
    }
   },
   "source": [
    ""
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T13:04:34.083981Z",
     "start_time": "2024-07-03T13:04:34.063104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import cv2\n",
    "import random as rd\n",
    "import numpy as np  \n",
    "\n",
    "\n",
    "def draw_keypoints_and_match(img1, img2):\n",
    "    \"\"\"This function is used for finding keypoints and dercriptors in the image and \n",
    "        find best matches using brute force/FLANN based matcher.\"\"\"\n",
    "\n",
    "    # Note : Can use sift too to improve feature extraction, but it can be patented again so it could brake the code in future!\n",
    "    # Note: ORB is not scale independent so number of keypoints depend on scale\n",
    "    # Initiate ORB detector\n",
    "    orb = cv2.ORB_create(nfeatures=10000)\n",
    "    # find the keypoints and descriptors with ORB\n",
    "    kp1, des1 = orb.detectAndCompute(img1,None)\n",
    "    kp2, des2 = orb.detectAndCompute(img2,None)\n",
    "\n",
    "    #________________________Brute Force Matcher_____________________________\n",
    "    # create BFMatcher object\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "    # Match descriptors.\n",
    "    matches = bf.match(des1,des2)\n",
    "\n",
    "    # Sort them in the order of their distance.\n",
    "    matches = sorted(matches, key = lambda x:x.distance)\n",
    "    # print(len(matches))\n",
    "    \n",
    "    # Select first 30 matches.\n",
    "    final_matches = matches[:30]\n",
    "\n",
    "    #___________________________________________________________________________\n",
    "\n",
    "    #________________________FLANN based Matcher________________________________\n",
    "    # FLANN_INDEX_LSH = 6\n",
    "    # index_params = dict(\n",
    "    #     algorithm=FLANN_INDEX_LSH,\n",
    "    #     table_number=6,  # 12\n",
    "    #     key_size=12,  # 20\n",
    "    #     multi_probe_level=1,\n",
    "    # )  # 2\n",
    "    # search_params = dict(checks=50)  # or pass empty dictionary\n",
    "    # flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "    # flann_match_pairs = flann.knnMatch(des1, des2, k=2)\n",
    "    \n",
    "    # # Filter matches using the Lowe's ratio test\n",
    "    # ratio_threshold = 0.3\n",
    "    # filtered_matches = []\n",
    "    # for m, n in flann_match_pairs:\n",
    "    #     if m.distance < ratio_threshold * n.distance:\n",
    "    #         filtered_matches.append(m)\n",
    "    \n",
    "    # print(\"FMatches\", len(filtered_matches))\n",
    "    # final_matches =  filtered_matches[:100]\n",
    "    #___________________________________________________________________________\n",
    "\n",
    "\n",
    "    # Draw keypoints\n",
    "    img_with_keypoints = cv2.drawMatches(img1,kp1,img2,kp2,final_matches,None,flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "    cv2.imwrite(\"images_with_matching_keypoints.png\", img_with_keypoints)\n",
    "\n",
    "    # Getting x,y coordinates of the matches\n",
    "    list_kp1 = [list(kp1[mat.queryIdx].pt) for mat in final_matches] \n",
    "    list_kp2 = [list(kp2[mat.trainIdx].pt) for mat in final_matches]\n",
    "\n",
    "    return list_kp1, list_kp2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_F_matrix(list_kp1, list_kp2):\n",
    "    \"\"\"This function is used to calculate the F matrix from a set of 8 points using SVD.\n",
    "        Furthermore, the rank of F matrix is reduced from 3 to 2 to make the epilines converge.\"\"\"\n",
    "\n",
    "    A = np.zeros(shape=(len(list_kp1), 9))\n",
    "\n",
    "    for i in range(len(list_kp1)):\n",
    "        x1, y1 = list_kp1[i][0], list_kp1[i][1]\n",
    "        x2, y2 = list_kp2[i][0], list_kp2[i][1]\n",
    "        A[i] = np.array([x1*x2, x1*y2, x1, y1*x2, y1*y2, y1, x2, y2, 1])\n",
    "\n",
    "    U, s, Vt = np.linalg.svd(A)\n",
    "    F = Vt[-1,:]\n",
    "    F = F.reshape(3,3)\n",
    "   \n",
    "    # Downgrading the rank of F matrix from 3 to 2\n",
    "    Uf, Df, Vft = np.linalg.svd(F)\n",
    "    Df[2] = 0\n",
    "    s = np.zeros((3,3))\n",
    "    for i in range(3):\n",
    "        s[i][i] = Df[i]\n",
    "\n",
    "    F = np.dot(Uf, np.dot(s, Vft))\n",
    "    return F\n",
    "\n",
    "def RANSAC_F_matrix(list_of_cood_list):\n",
    "    \"\"\"This method is used to shortlist the best F matrix using RANSAC based on the number of inliers.\"\"\"\n",
    "    \n",
    "    list_kp1 = list_of_cood_list[0]\n",
    "    list_kp2 = list_of_cood_list[1]\n",
    "    pairs = list(zip(list_kp1, list_kp2))  \n",
    "    max_inliers = 20\n",
    "    threshold = 0.05  # Tune this value\n",
    "  \n",
    "    for i in range(1000):\n",
    "        pairs = rd.sample(pairs, 8)  \n",
    "        rd_list_kp1, rd_list_kp2 = zip(*pairs) \n",
    "        F = calculate_F_matrix(rd_list_kp1, rd_list_kp2)\n",
    "        \n",
    "        tmp_inliers_img1 = []\n",
    "        tmp_inliers_img2 = []\n",
    "\n",
    "        for i in range(len(list_kp1)):\n",
    "            img1_x = np.array([list_kp1[i][0], list_kp1[i][1], 1])\n",
    "            img2_x = np.array([list_kp2[i][0], list_kp2[i][1], 1])\n",
    "            distance = abs(np.dot(img2_x.T, np.dot(F,img1_x)))\n",
    "            # print(distance)\n",
    "\n",
    "            if distance < threshold:\n",
    "                tmp_inliers_img1.append(list_kp1[i])\n",
    "                tmp_inliers_img2.append(list_kp2[i])\n",
    "\n",
    "        num_of_inliers = len(tmp_inliers_img1)\n",
    "        \n",
    "        # if num_of_inliers > inlier_count:\n",
    "        #     inlier_count = num_of_inliers\n",
    "        #     Best_F = F\n",
    "\n",
    "        if num_of_inliers > max_inliers:\n",
    "            print(\"Number of inliers\", num_of_inliers)\n",
    "            max_inliers = num_of_inliers\n",
    "            Best_F = F\n",
    "            inliers_img1 = tmp_inliers_img1\n",
    "            inliers_img2 = tmp_inliers_img2\n",
    "            # print(\"Best F matrix\", Best_F)\n",
    "\n",
    "    return Best_F\n",
    "\n",
    "\n",
    "def calculate_E_matrix(F, K1, K2):\n",
    "    \"\"\"Calculation of Essential matrix\"\"\"\n",
    "    \n",
    "    E = np.dot(K2.T, np.dot(F,K1))\n",
    "    return E\n",
    "\n",
    "\n",
    "def extract_camerapose(E):\n",
    "    \"\"\"This function extracts all the camera pose solutions from the E matrix\"\"\"\n",
    "\n",
    "    U, s, Vt = np.linalg.svd(E)\n",
    "    W = np.array([[0,-1, 0],\n",
    "                  [1, 0, 0],\n",
    "                  [0, 0, 1]])\n",
    "    \n",
    "    C1, C2 = U[:, 2], -U[:, 2]\n",
    "    R1, R2 = np.dot(U, np.dot(W,Vt)), np.dot(U, np.dot(W.T, Vt))\n",
    "    # print(\"C1\", C1, \"\\n\", \"C2\", C2, \"\\n\", \"R1\", R1, \"\\n\", \"R2\", R2, \"\\n\")\n",
    "    \n",
    "    camera_poses = [[R1, C1], [R1, C2], [R2, C1], [R2, C2]]\n",
    "    return camera_poses\n",
    "    \n",
    "\n",
    "def disambiguate_camerapose(camera_poses, list_kp1):\n",
    "    \"\"\"This fucntion is used to find the correct camera pose based on the chirelity condition from all 4 solutions.\"\"\"\n",
    "\n",
    "    max_len = 0\n",
    "    # Calculating 3D points \n",
    "    for pose in camera_poses:\n",
    "\n",
    "        front_points = []        \n",
    "        for point in list_kp1:\n",
    "            # Chirelity check\n",
    "            X = np.array([point[0], point[1], 1])\n",
    "            V = X - pose[1]\n",
    "            \n",
    "            condition = np.dot(pose[0][2], V)\n",
    "            if condition > 0:\n",
    "                front_points.append(point)    \n",
    "\n",
    "        if len(front_points) > max_len:\n",
    "            max_len = len(front_points)\n",
    "            best_camera_pose =  pose\n",
    "    \n",
    "    return best_camera_pose\n",
    "    \n",
    "\n",
    "def drawlines(img1src, img2src, lines, pts1src, pts2src):\n",
    "    \"\"\"This fucntion is used to visualize the epilines on the images\n",
    "        img1 - image on which we draw the epilines for the points in img2\n",
    "        lines - corresponding epilines \"\"\"\n",
    "    r, c = img1src.shape\n",
    "    img1color = cv2.cvtColor(img1src, cv2.COLOR_GRAY2BGR)\n",
    "    img2color = cv2.cvtColor(img2src, cv2.COLOR_GRAY2BGR)\n",
    "    # Edit: use the same random seed so that two images are comparable!\n",
    "    np.random.seed(0)\n",
    "    for r, pt1, pt2 in zip(lines, pts1src, pts2src):\n",
    "        color = tuple(np.random.randint(0, 255, 3).tolist())\n",
    "        x0, y0 = map(int, [0, -r[2]/r[1]])\n",
    "        x1, y1 = map(int, [c, -(r[2]+r[0]*c)/r[1]])\n",
    "        img1color = cv2.line(img1color, (x0, y0), (x1, y1), color, 1)\n",
    "        img1color = cv2.circle(img1color, tuple(pt1), 5, color, -1)\n",
    "        img2color = cv2.circle(img2color, tuple(pt2), 5, color, -1)\n",
    "    \n",
    "    return img1color, img2color"
   ],
   "id": "22ce55a1d60edfc0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T13:04:34.095732Z",
     "start_time": "2024-07-03T13:04:34.085988Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def sum_of_squared_diff(pixel_vals_1, pixel_vals_2):\n",
    "    \"\"\"Sum of squared distances for Correspondence\"\"\"\n",
    "    if pixel_vals_1.shape != pixel_vals_2.shape:\n",
    "        return -1\n",
    "\n",
    "    return np.sum((pixel_vals_1 - pixel_vals_2)**2)\n",
    "\n",
    "def block_comparison(y, x, block_left, right_array, block_size, x_search_block_size, y_search_block_size):\n",
    "    \"\"\"Block comparison function used for comparing windows on left and right images and find the minimum value ssd match the pixels\"\"\"\n",
    "    \n",
    "    # Get search range for the right image\n",
    "    x_min = max(0, x - x_search_block_size)\n",
    "    x_max = min(right_array.shape[1], x + x_search_block_size)\n",
    "    y_min = max(0, y - y_search_block_size)\n",
    "    y_max = min(right_array.shape[0], y + y_search_block_size)\n",
    "    \n",
    "    first = True\n",
    "    min_ssd = None\n",
    "    min_index = None\n",
    "\n",
    "    for y in range(y_min, y_max):\n",
    "        for x in range(x_min, x_max):\n",
    "            block_right = right_array[y: y+block_size, x: x+block_size]\n",
    "            ssd = sum_of_squared_diff(block_left, block_right)\n",
    "            if first:\n",
    "                min_ssd = ssd\n",
    "                min_index = (y, x)\n",
    "                first = False\n",
    "            else:\n",
    "                if ssd < min_ssd:\n",
    "                    min_ssd = ssd\n",
    "                    min_index = (y, x)\n",
    "\n",
    "    return min_index\n",
    "\n",
    "\n",
    "def ssd_correspondence(img1, img2):\n",
    "    \"\"\"Correspondence applied on the whole image to compute the disparity map and finally disparity map is scaled\"\"\"\n",
    "    # Don't search full line for the matching pixel\n",
    "    # grayscale imges\n",
    "\n",
    "    block_size = 15 # 15\n",
    "    x_search_block_size = 50 # 50 \n",
    "    y_search_block_size = 1\n",
    "    h, w = img1.shape\n",
    "    disparity_map = np.zeros((h, w))\n",
    "\n",
    "    for y in range(block_size, h-block_size):\n",
    "        for x in range(block_size, w-block_size):\n",
    "            block_left = img1[y:y + block_size, x:x + block_size]\n",
    "            index = block_comparison(y, x, block_left, img2, block_size, x_search_block_size, y_search_block_size)\n",
    "            disparity_map[y, x] = abs(index[1] - x)\n",
    "    \n",
    "    disparity_map_unscaled = disparity_map.copy()\n",
    "\n",
    "    # Scaling the disparity map\n",
    "    max_pixel = np.max(disparity_map)\n",
    "    min_pixel = np.min(disparity_map)\n",
    "\n",
    "    for i in range(disparity_map.shape[0]):\n",
    "        for j in range(disparity_map.shape[1]):\n",
    "            disparity_map[i][j] = int((disparity_map[i][j]*255)/(max_pixel-min_pixel))\n",
    "    \n",
    "    disparity_map_scaled = disparity_map\n",
    "    return disparity_map_unscaled, disparity_map_scaled"
   ],
   "id": "56b4deb70344ccf3",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T13:04:34.103645Z",
     "start_time": "2024-07-03T13:04:34.098728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def disparity_to_depth(baseline, f, img):\n",
    "    \"\"\"This is used to compute the depth values from the disparity map\"\"\"\n",
    "\n",
    "    # Assumption image intensities are disparity values (x-x') \n",
    "    depth_map = np.zeros((img.shape[0], img.shape[1]))\n",
    "    depth_array = np.zeros((img.shape[0], img.shape[1]))\n",
    "\n",
    "    for i in range(depth_map.shape[0]):\n",
    "        for j in range(depth_map.shape[1]):\n",
    "            depth_map[i][j] = 1/img[i][j]\n",
    "            depth_array[i][j] = baseline*f/img[i][j]\n",
    "            # if math.isinf(depth_map[i][j]):\n",
    "            #     depth_map[i][j] = 1\n",
    "\n",
    "    return depth_map, depth_array"
   ],
   "id": "c917661a28f20d01",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-03T13:04:34.167150Z",
     "start_time": "2024-07-03T13:04:34.105648Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import cv2\n",
    "import random as rd\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from calibration import draw_keypoints_and_match, drawlines, RANSAC_F_matrix, calculate_E_matrix, extract_camerapose, disambiguate_camerapose\n",
    "from rectification import rectification\n",
    "from correspondence import ssd_correspondence\n",
    "from depth import disparity_to_depth\n",
    "\n",
    "# Its all about resolution - Its a trade off between resolution and time of computation\n",
    "\n",
    "def main():\n",
    "    number = int(input(\"Please enter the dataset number (1/2/3) to use for calculating the depth map\\n\"))\n",
    "    img1 = cv2.imread(f\"Dataset{number}/im0.png\", 0)\n",
    "    img2 = cv2.imread(f\"Dataset{number}/im1.png\", 0)\n",
    "\n",
    "    width = int(img1.shape[1]* 0.3) # 0.3\n",
    "    height = int(img1.shape[0]* 0.3) # 0.3\n",
    "\n",
    "    img1 = cv2.resize(img1, (width, height), interpolation = cv2.INTER_AREA)\n",
    "    # img1 = cv2.GaussianBlur(img1,(5,5),0)\n",
    "    img2 = cv2.resize(img2, (width, height), interpolation = cv2.INTER_AREA)\n",
    "    # img2 = cv2.GaussianBlur(img2,(5,5),0)\n",
    "    \n",
    "    #__________________Camera Parameters________________________________\n",
    "    K11 = np.array([[5299.313,  0,   1263.818], \n",
    "                [0,      5299.313, 977.763],\n",
    "                [0,          0,       1   ]])\n",
    "    K12 = np.array([[5299.313,   0,    1438.004],\n",
    "                [0,      5299.313,  977.763 ],\n",
    "                [0,           0,      1     ]])\n",
    "\n",
    "    K21 = np.array([[4396.869, 0, 1353.072],\n",
    "                    [0, 4396.869, 989.702],\n",
    "                    [0, 0, 1]])\n",
    "    K22 = np.array([[4396.869, 0, 1538.86],\n",
    "                [0, 4396.869, 989.702],\n",
    "                [0, 0, 1]])\n",
    "    \n",
    "    K31 = np.array([[5806.559, 0, 1429.219],\n",
    "                    [0, 5806.559, 993.403],\n",
    "                    [ 0, 0, 1]])\n",
    "    K32 = np.array([[5806.559, 0, 1543.51],\n",
    "                    [ 0, 5806.559, 993.403],\n",
    "                    [ 0, 0, 1]])\n",
    "    camera_params = [(K11, K12), (K21, K22), (K31, K32)]\n",
    "\n",
    "    while(1):\n",
    "        try:\n",
    "            list_kp1, list_kp2 = draw_keypoints_and_match(img1, img2)\n",
    "            \n",
    "            #_______________________________Calibration_______________________________\n",
    "\n",
    "            F = RANSAC_F_matrix([list_kp1, list_kp2])\n",
    "            print(\"F matrix\", F)\n",
    "            print(\"==\"*20, '\\n')\n",
    "            K1, K2 = camera_params[number-1]\n",
    "            E = calculate_E_matrix(F, K1, K2)\n",
    "            print(\"E matrix\", E)\n",
    "            print(\"==\"*20, '\\n')\n",
    "            camera_poses = extract_camerapose(E)\n",
    "            best_camera_pose = disambiguate_camerapose(camera_poses, list_kp1)\n",
    "            print(\"Best_Camera_Pose:\")\n",
    "            print(\"==\"*20)\n",
    "            print(\"Roatation\", best_camera_pose[0])\n",
    "            print()\n",
    "            print(\"Transaltion\", best_camera_pose[1])\n",
    "            print(\"==\"*20, '\\n')\n",
    "            pts1 = np.int32(list_kp1)\n",
    "            pts2 = np.int32(list_kp2)\n",
    "\n",
    "            #____________________________Rectification________________________________\n",
    "            \n",
    "            rectified_pts1, rectified_pts2, img1_rectified, img2_rectified = rectification(img1, img2, pts1, pts2, F)\n",
    "            break\n",
    "        except Exception as e:\n",
    "            # print(\"error\", e)\n",
    "            continue\n",
    "    \n",
    "    # Find epilines corresponding to points in right image (second image) and drawing its lines on left image\n",
    "    \n",
    "    lines1 = cv2.computeCorrespondEpilines(rectified_pts2.reshape(-1, 1, 2), 2, F)\n",
    "    lines1 = lines1.reshape(-1, 3)\n",
    "    img5, img6 = drawlines(img1_rectified, img2_rectified, lines1, rectified_pts1, rectified_pts2)\n",
    "\n",
    "    # Find epilines corresponding to points in left image (first image) and drawing its lines on right image\n",
    "\n",
    "    lines2 = cv2.computeCorrespondEpilines(rectified_pts1.reshape(-1, 1, 2), 1, F)\n",
    "    lines2 = lines2.reshape(-1, 3)\n",
    "    img3, img4 = drawlines(img2_rectified, img1_rectified, lines2, rectified_pts2, rectified_pts1)\n",
    "\n",
    "    cv2.imwrite(\"left_image.png\", img5)\n",
    "    cv2.imwrite(\"right_image.png\", img3)\n",
    "    \n",
    "    #____________________________Correspondance________________________________\n",
    "    \n",
    "    disparity_map_unscaled, disparity_map_scaled = ssd_correspondence(img1_rectified, img2_rectified)\n",
    "    # cv2.imwrite(f\"disparity_map_{number}.png\", disparity_map_scaled)\n",
    "\n",
    "    # img_n = cv2.normalize(src=disparity_map_scaled, dst=None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_8U)\n",
    "    # heatmap1 = cv2.applyColorMap(img_n, cv2.COLORMAP_HOT)\n",
    "    # cv2.imwrite(f\"disparity_heat_map_{number}.png\", heatmap1)\n",
    "    plt.figure(1)\n",
    "    plt.title('Disparity Map Graysacle')\n",
    "    plt.imshow(disparity_map_scaled, cmap='gray')\n",
    "    plt.figure(2)\n",
    "    plt.title('Disparity Map Hot')\n",
    "    plt.imshow(disparity_map_scaled, cmap='hot')\n",
    "    \n",
    "\n",
    "    #________________________________Depth______________________________________\n",
    "    baseline1, f1 = 177.288, 5299.313\n",
    "    baseline2, f2 = 144.049, 4396.869\n",
    "    baseline3, f3 = 174.019, 5806.559\n",
    "    \n",
    "    params = [(baseline1, f1), (baseline2, f2), (baseline3, f3)]\n",
    "    baseline, f = params[number-1]\n",
    "    depth_map, depth_array = disparity_to_depth(baseline, f, disparity_map_unscaled)\n",
    "    \n",
    "    plt.figure(3)\n",
    "    plt.title('Depth Map Graysacle')\n",
    "    plt.imshow(depth_map, cmap='gray')\n",
    "    plt.figure(4)\n",
    "    plt.title('Depth Map Hot')\n",
    "    plt.imshow(depth_map, cmap='hot')\n",
    "    plt.show()\n",
    "\n",
    "    print(\"==\"*20)\n",
    "    # print(\"Depth values\", depth_array)\n",
    "\n",
    "    #____________________________________________________________________________\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ],
   "id": "4cc06c218a0b4f53",
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31merror\u001B[0m                                     Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m  \n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcalibration\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m draw_keypoints_and_match, drawlines, RANSAC_F_matrix, calculate_E_matrix, extract_camerapose, disambiguate_camerapose\n\u001B[0;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mrectification\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m rectification\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcorrespondence\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ssd_correspondence\n",
      "File \u001B[1;32m~\\Documents\\GitHub\\AKAT-3D-Analyzator\\calibration.py:21\u001B[0m\n\u001B[0;32m     18\u001B[0m image \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mimread(image_path)\n\u001B[0;32m     20\u001B[0m \u001B[38;5;66;03m# Convert the image to grayscale\u001B[39;00m\n\u001B[1;32m---> 21\u001B[0m gray \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mcvtColor(image, cv2\u001B[38;5;241m.\u001B[39mCOLOR_BGR2GRAY)\n\u001B[0;32m     23\u001B[0m \u001B[38;5;66;03m# Find the chessboard corners\u001B[39;00m\n\u001B[0;32m     24\u001B[0m ret, corners \u001B[38;5;241m=\u001B[39m cv2\u001B[38;5;241m.\u001B[39mfindChessboardCorners(gray, chessboard_size, \u001B[38;5;28;01mNone\u001B[39;00m)\n",
      "\u001B[1;31merror\u001B[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\color.cpp:196: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def rectification(img1, img2, pts1, pts2, F):\n",
    "    \"\"\"This function is used to rectify the images to make camera pose's parallel and thus make epiplines as horizontal.\n",
    "        Since camera distortion parameters are not given we will use cv2.stereoRectifyUncalibrated(), instead of stereoRectify().\n",
    "    \"\"\"\n",
    "\n",
    "    # Stereo rectification\n",
    "    h1, w1 = img1.shape\n",
    "    h2, w2 = img2.shape\n",
    "\n",
    "    _, H1, H2 = cv2.stereoRectifyUncalibrated(np.float32(pts1), np.float32(pts2), F, imgSize=(w1, h1))\n",
    "    print(\"H1\",H1)\n",
    "    print(\"H2\",H2)\n",
    "\n",
    "    rectified_pts1 = np.zeros((pts1.shape), dtype=int)\n",
    "    rectified_pts2 = np.zeros((pts2.shape), dtype=int)\n",
    "\n",
    "    # Rectify the feature points\n",
    "    for i in range(pts1.shape[0]):\n",
    "        source1 = np.array([pts1[i][0], pts1[i][1], 1])\n",
    "        new_point1 = np.dot(H1, source1)\n",
    "        new_point1[0] = int(new_point1[0]/new_point1[2])\n",
    "        new_point1[1] = int(new_point1[1]/new_point1[2])\n",
    "        new_point1 = np.delete(new_point1, 2)\n",
    "        rectified_pts1[i] = new_point1\n",
    "\n",
    "        source2 = np.array([pts2[i][0], pts2[i][1], 1])\n",
    "        new_point2 = np.dot(H2, source2)\n",
    "        new_point2[0] = int(new_point2[0]/new_point2[2])\n",
    "        new_point2[1] = int(new_point2[1]/new_point2[2])\n",
    "        new_point2 = np.delete(new_point2, 2)\n",
    "        rectified_pts2[i] = new_point2\n",
    "\n",
    "    # Rectify the images and save them\n",
    "    img1_rectified = cv2.warpPerspective(img1, H1, (w1, h1))\n",
    "    img2_rectified = cv2.warpPerspective(img2, H2, (w2, h2))\n",
    "    \n",
    "    cv2.imwrite(\"rectified_1.png\", img1_rectified)\n",
    "    cv2.imwrite(\"rectified_2.png\", img2_rectified)\n",
    "    \n",
    "    return rectified_pts1, rectified_pts2, img1_rectified, img2_rectified\n",
    "\n",
    "\n",
    "   "
   ],
   "id": "9f569d6b0089a39d",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
